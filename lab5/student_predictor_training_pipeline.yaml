# Name: student-predictor-training-pipeline-v2
# Inputs:
#    data_bucket: str
#    model_repo: str
#    project_id: str
#    testset_filename: str
#    trainset_filename: str
components:
  comp-download-data:
    executorLabel: exec-download-data
    inputDefinitions:
      parameters:
        bucket:
          parameterType: STRING
        file_name:
          parameterType: STRING
        project_id:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-mlp:
    executorLabel: exec-train-mlp
    inputDefinitions:
      parameters:
        project_id:
          parameterType: STRING
        feature_path:
          parameterType: STRING
        model_repo:
          parameterType: STRING
        metrics_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        metrics:
          parameterType: STRUCT
  comp-upload-model-to-gcs:
    executorLabel: exec-upload-model-to-gcs
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_repo:
          parameterType: STRING
        project_id:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-download-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_data
        command:
        - sh
        - -c
        - |
          if ! [ -x "$(command -v pip)" ]; then
              python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
          fi
          PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<"3.9"'  && python3 -m pip install --quiet --no-warn-script-location 'pandas' 'google-cloud-storage' && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp -d)
          printf "%s" "$0" > "$program_path/ephemeral_component.py"
          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main --component_module_path "$program_path/ephemeral_component.py" "$@"
        - |
          import kfp
          from kfp import dsl
          from kfp.dsl import *
          from typing import *
          
          def download_data(project_id: str, bucket: str, file_name: str, dataset: Output[Dataset]):
              '''download data'''
              from google.cloud import storage
              import pandas as pd
              import logging 
              import sys
              
              logging.basicConfig(stream=sys.stdout, level=logging.INFO)
              
              # Downloading the file from a google bucket 
              client = storage.Client(project=project_id)
              bucket = client.bucket(bucket)
              blob = bucket.blob(file_name)
              blob.download_to_filename(dataset.path + ".csv")
              logging.info('Downloaded Data!')
        image: python:3.10.7-slim
    exec-train-mlp:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_mlp
        command:
        - sh
        - -c
        - |
          if ! [ -x "$(command -v pip)" ]; then
              python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
          fi
          PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<"3.9"'  && python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn' 'flask' 'logging' 'pathlib' 'google-cloud-storage' && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp -d)
          printf "%s" "$0" > "$program_path/ephemeral_component.py"
          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main --component_module_path "$program_path/ephemeral_component.py" "$@"
        - |
          import argparse
          import logging
          import os
          import pickle
          import sys
          from pathlib import Path

          import pandas as pd
          from sklearn.compose import ColumnTransformer
          from sklearn.ensemble import StackingClassifier
          from sklearn.linear_model import LogisticRegression
          from sklearn.metrics import accuracy_score, classification_report
          from sklearn.model_selection import StratifiedKFold, train_test_split
          from sklearn.pipeline import Pipeline
          from sklearn.preprocessing import OneHotEncoder, StandardScaler
          from sklearn.svm import SVC
          from google.cloud import storage
          from flask import jsonify

          import json

          def train_mlp(project_id, feature_path, model_repo, metrics_path):
              logging.basicConfig(stream=sys.stdout, level=logging.INFO)

              df = pd.read_csv(feature_path, index_col=None)

              logging.info(df.columns)

              # Define feature set X and target variable y
              categorical_columns = ['schoolsup', 'higher']
              numerical_columns = ['absences', 'failures', 'Medu', 'Fedu', 'Walc', 'Dalc', 'famrel', 'goout', 'freetime', 'studytime']
              columns_to_average = ['G1', 'G2', 'G3']
              df['Average_Grade'] = df[columns_to_average].mean(axis=1)
              df['Average_Grade_Cat_1'] = pd.cut(df['Average_Grade'], bins=[0, 10, 20], labels=[0, 1], include_lowest=True)
              X = df[numerical_columns + categorical_columns]
              y = df['Average_Grade_Cat_1']

              # Split data into training and test sets
              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

              # Define pre-processing steps
              preprocessor = ColumnTransformer(
                  transformers=[
                      ('num', StandardScaler(), numerical_columns),
                      ('cat', OneHotEncoder(drop='if_binary'), categorical_columns)
                  ])

              # Define the base models
              svm_linear_clf = SVC(C=0.1, kernel='linear', gamma='scale', class_weight='balanced', degree=2, probability=True)
              svm_rbf_clf = SVC(C=0.1, kernel='rbf', gamma='scale', class_weight='balanced', degree=2, probability=True)

              # Meta-classifier
              logreg = LogisticRegression(C=10, solver='newton-cg')

              # Stacking ensemble classifier configuration
              stack_clf = StackingClassifier(
                  estimators=[
                      ('svm_linear_clf', svm_linear_clf),
                      ('svm_rbf', svm_rbf_clf)
                  ],
                  final_estimator=logreg,
                  cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
              )

              # Create the pipeline
              pipeline = Pipeline(steps=[
                  ('preprocessor', preprocessor),
                  ('classifier', stack_clf)
              ])

              # Fit the pipeline to the training data
              logging.info("Training the model with the following configuration...")
              pipeline.fit(X_train, y_train)

              # Evaluate the model
              y_pred = pipeline.predict(X_test)
              accuracy = accuracy_score(y_test, y_pred)
              logging.info(f"Test Accuracy: {accuracy}")
              print(f"Test Accuracy: {accuracy}")
              print(classification_report(y_test, y_pred))

              # Save the model as a .pkl file in a local path
              local_file = '/tmp/model_train.pkl'  # Local path to save the model

              with open(local_file, 'wb') as f:
                  pickle.dump(pipeline, f)

              logging.info(f"Model saved to {local_file}")

              # Save to GCS as model.pkl
              client = storage.Client(project=project_id)
              bucket = client.get_bucket(model_repo)
              blob = bucket.blob('model.pkl')
              # Upload the locally saved model
              blob.upload_from_filename(local_file)
              # Clean up
              os.remove(local_file)
              logging.info("Saved the model to GCP bucket : " + model_repo)

              # Creating the directory where the output file is created (the directory
              # may or may not exist).
              Path(metrics_path).parent.mkdir(parents=True, exist_ok=True)
              with open(metrics_path, 'w') as outfile:
                  json.dump({"accuracy": accuracy}, outfile)

          # Defining and parsing the command-line arguments
          def parse_command_line_arguments():
              parser = argparse.ArgumentParser()
              parser.add_argument('--project_id', type=str, help="GCP project id")
              parser.add_argument('--feature_path', type=str, help="CSV file with features")
              parser.add_argument('--model_repo', type=str, help="Name of the model bucket")
              parser.add_argument('--metrics_path', type=str, help="Name of the file to be used for saving evaluation metrics")
              args = parser.parse_args()
              return vars(args)

          if __name__ == '__main__':
              train_mlp(**parse_command_line_arguments())
        image: python:3.10.7-slim
    exec-upload-model-to-gcs:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_gcs
        command:
        - sh
        - -c
        - |
          if ! [ -x "$(command -v pip)" ]; then
              python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
          fi
          PIP_DISABLE_PIP_VERSION_CHECK=1 && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp -d)
          printf "%s" "$0" > "$program_path/ephemeral_component.py"
          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main --component_module_path "$program_path/ephemeral_component.py" "$@"
        - |
          import kfp
          from kfp import dsl
          from kfp.dsl import *
          from typing import *
          
          def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):
              '''upload model to GCS'''
              from google.cloud import storage
              import logging
              import sys
              
              logging.basicConfig(stream=sys.stdout, level=logging.INFO)
              
              client = storage.Client(project=project_id)
              bucket = client.bucket(model_repo)
              blob = bucket.blob(model.path.split('/')[-1])
              blob.upload_from_filename(model.path)
              logging.info(f'Model uploaded to {model_repo}')
pipelineInfo:
  name: student-predictor-training-pipeline-v2
root:
  dag:
    tasks:
      download-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-data
        inputs:
          parameters:
            bucket:
              componentInputParameter: data_bucket
            file_name:
              componentInputParameter: trainset_filename
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: download-data
      train-mlp:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-mlp
        dependentTasks:
        - download-data
        inputs:
          parameters:
            project_id:
              componentInputParameter: project_id
            feature_path:
              taskOutputArtifact:
                outputArtifactKey: dataset
                producerTask: download-data
            model_repo:
              componentInputParameter: model_repo
            metrics_path:
              componentInputParameter: metrics_path
        taskInfo:
          name: train-mlp
      upload-model-to-gcs:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-upload-model-to-gcs
        dependentTasks:
        - train-mlp
        inputs:
          artifacts:
            model:
              taskOutputArtifact:
                outputArtifactKey: out_model
                producerTask: train-mlp
          parameters:
            model_repo:
              componentInputParameter: model_repo
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: upload-model-to-gcs
  inputDefinitions:
    parameters:
      data_bucket:
        parameterType: STRING
      model_repo:
        parameterType: STRING
      project_id:
        parameterType: STRING
      testset_filename:
        parameterType: STRING
      trainset_filename:
        parameterType: STRING
      metrics_path:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
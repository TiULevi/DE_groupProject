# Name: student-predictor-training-pipeline-v2
# Inputs:
#    data_bucket: str
#    model_repo: str
#    project_id: str
#    testset_filename: str
#    trainset_filename: str
components:
  comp-download-data:
    executorLabel: exec-download-data
    inputDefinitions:
      parameters:
        bucket:
          parameterType: STRING
        file_name:
          parameterType: STRING
        project_id:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-mlp:
    executorLabel: exec-train-mlp
    inputDefinitions:
      artifacts:
        features:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        metrics:
          parameterType: STRUCT
  comp-upload-model-to-gcs:
    executorLabel: exec-upload-model-to-gcs
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_repo:
          parameterType: STRING
        project_id:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-download-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_data
        command:
        - sh
        - -c
        - |
          if ! [ -x "$(command -v pip)" ]; then
              python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
          fi
          PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<"3.9"'  && python3 -m pip install --quiet --no-warn-script-location 'pandas' 'google-cloud-storage' && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp -d)
          printf "%s" "$0" > "$program_path/ephemeral_component.py"
          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main --component_module_path "$program_path/ephemeral_component.py" "$@"
        - |
          import kfp
          from kfp import dsl
          from kfp.dsl import *
          from typing import *
          
          def download_data(project_id: str, bucket: str, file_name: str, dataset: Output[Dataset]):
              '''download data'''
              from google.cloud import storage
              import pandas as pd
              import logging 
              import sys
              
              logging.basicConfig(stream=sys.stdout, level=logging.INFO)
              
              # Downloading the file from a google bucket 
              client = storage.Client(project=project_id)
              bucket = client.bucket(bucket)
              blob = bucket.blob(file_name)
              blob.download_to_filename(dataset.path + ".csv")
              logging.info('Downloaded Data!')
        image: python:3.10.7-slim
    exec-train-mlp:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_mlp
        command:
        - sh
        - -c
        - |
          if ! [ -x "$(command -v pip)" ]; then
              python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
          fi
          PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<"3.9"'  && python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn' 'flask' 'logging' 'pathlib' && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp -d)
          printf "%s" "$0" > "$program_path/ephemeral_component.py"
          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main --component_module_path "$program_path/ephemeral_component.py" "$@"
        - |
          import kfp
          from kfp import dsl
          from kfp.dsl import *
          from typing import *
          
          def train_mlp(features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):
              '''train a MLP with default parameters'''
              import pandas as pd
              import logging
              import sys
              import os
              import pickle
              from sklearn.compose import ColumnTransformer
              from sklearn.ensemble import StackingClassifier
              from sklearn.linear_model import LogisticRegression
              from sklearn.metrics import accuracy_score, classification_report
              from sklearn.model_selection import StratifiedKFold, train_test_split
              from sklearn.pipeline import Pipeline
              from sklearn.preprocessing import OneHotEncoder, StandardScaler
              from sklearn.svm import SVC
              from flask import jsonify
              
              logging.basicConfig(stream=sys.stdout, level=logging.INFO)
              
              df = pd.read_csv(features.path+".csv")
              
              logging.info(df.columns)
              
              # Define feature set X and target variable y
              categorical_columns = ['schoolsup', 'higher']
              numerical_columns = ['absences', 'failures', 'Medu', 'Fedu', 'Walc', 'Dalc', 'famrel', 'goout', 'freetime', 'studytime']
              columns_to_average = ['G1', 'G2', 'G3']
              df['Average_Grade'] = df[columns_to_average].mean(axis=1)
              df['Average_Grade_Cat_1'] = pd.cut(df['Average_Grade'], bins=[0, 10, 20], labels=[0, 1], include_lowest=True)
              X = df[numerical_columns + categorical_columns]
              y = df['Average_Grade_Cat_1']
              
              # Split data into training and test sets
              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
              
              # Define pre-processing steps
              preprocessor = ColumnTransformer(
                  transformers=[
                      ('num', StandardScaler(), numerical_columns),
                      ('cat', OneHotEncoder(drop='if_binary'), categorical_columns)
                  ])
              
              # Define the base models
              svm_linear_clf = SVC(C=0.1, kernel='linear', gamma='scale', class_weight='balanced', degree=2, probability=True)
              svm_rbf_clf = SVC(C=0.1, kernel='rbf', gamma='scale', class_weight='balanced', degree=2, probability=True)
              
              # Meta-classifier
              logreg = LogisticRegression(C=10, solver='newton-cg')
              
              # Stacking ensemble classifier configuration
              stack_clf = StackingClassifier(
                  estimators=[
                      ('svm_linear_clf', svm_linear_clf),
                      ('svm_rbf', svm_rbf_clf)
                  ],
                  final_estimator=logreg,
                  cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
              )
              
              # Create the pipeline
              pipeline = Pipeline(steps=[
                  ('preprocessor', preprocessor),
                  ('classifier', stack_clf)
              ])
              
              # Fit the pipeline to the training data
              logging.info("Training the model with the following configuration...")
              pipeline.fit(X_train, y_train)
              
              # Evaluate the model
              y_pred = pipeline.predict(X_test)
              accuracy = accuracy_score(y_test, y_pred)
              logging.info(f"Test Accuracy: {accuracy}")
              print(f"Test Accuracy: {accuracy}")
              print(classification_report(y_test, y_pred))
              
              # Save the model as a .pkl file in a local path
              local_file = '/tmp/model_train.pkl'  # Local path to save the model
              
              with open(local_file, 'wb') as f:
                  pickle.dump(pipeline, f)
              
              logging.info(f"Model saved to {local_file}")
              
              out_model.metadata["file_type"] = ".pkl"
              out_model.metadata["algo"] = "mlp"
              # Save the model
              m_file = out_model.path + ".pkl"
              with open(m_file, 'wb') as f:
                  pickle.dump(pipeline, f)
              
              metrics_dict = {
                  "accuracy": accuracy
              }
              
              outputs = NamedTuple('outputs', metrics=dict)
              return outputs(metrics_dict)
        image: python:3.10.7-slim
    exec-upload-model-to-gcs:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_gcs
        command:
        - sh
        - -c
        - |
          if ! [ -x "$(command -v pip)" ]; then
              python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
          fi
          PIP_DISABLE_PIP_VERSION_CHECK=1 && "$0" "$@"
        - sh
        - -ec
        - |
          program_path=$(mktemp -d)
          printf "%s" "$0" > "$program_path/ephemeral_component.py"
          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main --component_module_path "$program_path/ephemeral_component.py" "$@"
        - |
          import kfp
          from kfp import dsl
          from kfp.dsl import *
          from typing import *
          
          def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):
              '''upload model to GCS'''
              from google.cloud import storage
              import logging
              import sys
              
              logging.basicConfig(stream=sys.stdout, level=logging.INFO)
              
              client = storage.Client(project=project_id)
              bucket = client.bucket(model_repo)
              blob = bucket.blob(model.path.split('/')[-1])
              blob.upload_from_filename(model.path)
              logging.info(f'Model uploaded to {model_repo}')
pipelineInfo:
  name: student-predictor-training-pipeline-v2
root:
  dag:
    tasks:
      download-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-data
        inputs:
          parameters:
            bucket: {inputValue: data_bucket}
            file_name: {inputValue: trainset_filename}
            project_id: {inputValue: project_id}
        taskInfo:
          name: download-data
      train-mlp:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-mlp
        dependentTasks:
        - download-data
        inputs:
          artifacts:
            features: {taskOutputArtifact: download-data.dataset}
        taskInfo:
          name: train-mlp
      upload-model-to-gcs:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-upload-model-to-gcs
        dependentTasks:
        - train-mlp
        inputs:
          artifacts:
            model: {taskOutputArtifact: train-mlp.out_model}
          parameters:
            model_repo: {inputValue: model_repo}
            project_id: {inputValue: project_id}
        taskInfo:
          name: upload-model-to-gcs
  inputDefinitions:
    parameters:
      data_bucket:
        parameterType: STRING
      model_repo:
        parameterType: STRING
      project_id:
        parameterType: STRING
      testset_filename:
        parameterType: STRING
      trainset_filename:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0